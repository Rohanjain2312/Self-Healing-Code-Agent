{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Self-Healing Code Agent — Colab Launcher\n",
    "\n",
    "Runs the Self-Healing Code Agent on a Colab GPU and exposes a public Gradio link.\n",
    "\n",
    "**Recommended runtime:** GPU → T4 (free) or A100 (Colab Pro)\n",
    "\n",
    "Steps:\n",
    "1. **Runtime → Change runtime type → GPU (T4)**\n",
    "2. Run all cells (Runtime → Run all)\n",
    "3. Copy the `gradio.live` public URL from Cell 3 output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# ── Cell 1: GPU check ───────────────────────────────────────────────────────\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"⚠️  No GPU detected — switch Runtime to T4 GPU for best results.\")\n",
    "    print(\"    Runtime → Change runtime type → Hardware accelerator → GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# ── Cell 2: Clone repo and install dependencies ──────────────────────────────\n",
    "!git clone https://github.com/Rohanjain2312/Self-Healing-Code-Agent.git\n",
    "%cd Self-Healing-Code-Agent\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q transformers torch accelerate\n",
    "print(\"✅ Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "launch"
   },
   "outputs": [],
   "source": [
    "# ── Cell 3: Configure environment and launch Gradio demo ─────────────────────\n",
    "#\n",
    "# GPU model selection:\n",
    "#   T4  (free tier, ~15 GB VRAM)  → keep HF_MODEL as 3B-Instruct (default)\n",
    "#   A100/V100 (Colab Pro)         → change to Llama-3.1-8B-Instruct\n",
    "#\n",
    "import os, sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "os.environ['LLM_PROVIDER'] = 'huggingface'\n",
    "\n",
    "# ── Uncomment the line that matches your GPU ──\n",
    "# os.environ['HF_MODEL'] = 'meta-llama/Llama-3.1-8B-Instruct'  # A100 / V100\n",
    "os.environ.setdefault('HF_MODEL', 'meta-llama/Llama-3.2-3B-Instruct')  # T4 default\n",
    "\n",
    "from demo.app import build_app\n",
    "app = build_app()\n",
    "# share=True publishes a temporary public gradio.live URL (valid for 72 h)\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark"
   },
   "outputs": [],
   "source": [
    "# ── Cell 4 (Optional): Run the benchmark instead of the demo ─────────────────\n",
    "#\n",
    "# Skip this cell if you just want the Gradio UI from Cell 3.\n",
    "# This runs all 8 benchmark tasks and writes results to evaluation/results.json.\n",
    "#\n",
    "!python -m evaluation.run_benchmark --provider huggingface --max-iterations 4"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
